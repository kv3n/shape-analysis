{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "shape-preserving-transformations.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kv3n/shape-analysis/blob/master/shape_preserving_transformations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dXwd9Y36rGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-graphics\n",
        "!pip install openmesh\n",
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Snjyi0wu7BmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow_graphics.geometry.transformation import quaternion\n",
        "from tensorflow_graphics.notebooks import mesh_viewer\n",
        "import openmesh as om\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrWiQCyZzPAs",
        "colab_type": "text"
      },
      "source": [
        "# Loading the Model\n",
        "\n",
        "I experimented a few ways to do this in Google Colab, but I had to settle for a solution that involves a little bit of work. We are going to have to upload a model to a specific location in Google Drive for this to work.\n",
        "\n",
        "**Steps**:\n",
        "- Create a folder called resources in your Google Drive.\n",
        "- Upload a 3D model file (I downloaded mine from [Google Poly by \"Keen Chan\"](https://poly.google.com/view/caZFn1bu4Zc)). We only care about the .obj file for now.\n",
        "- Rename the 3D model file to 'model.obj'\n",
        "- Authorize google drive access when running the cell below\n",
        "\n",
        ".. and with that you should be ready to load and view the model.\n",
        "\n",
        "  \n",
        "Feel free to alter this block however you feel fit to load the openmesh model!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juy9lA8j_Sz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Create\n",
        "\n",
        "mesh = om.read_trimesh('/content/drive/My Drive/resources/model.obj')\n",
        "print(\"Mesh Loaded\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-8wSgOyz5Nl",
        "colab_type": "text"
      },
      "source": [
        "# Build Tensorflow Ready Mesh Data\n",
        "Tensorflow expects a mesh to be definied as a JSON dicitonary with the following format: \n",
        "```\n",
        "  { \n",
        "    'vertices': <array of 3D positions>,\n",
        "     'faces': <array of 3 indices of vertices that form a traingle>,\n",
        "      'vertex_colors': <array of colors for each vertex>,\n",
        "      'material': <a material applied to the model>\n",
        "  }\n",
        "```\n",
        "\n",
        "The vertex_colors and materials are optional. So we will ignore them for now and keep our demo simple. \n",
        "\n",
        "I use OpenMesh here to load our 3D model, hence the following cell of code simply just converts an open mesh loaded 3D object into tensorflow 3D mesh data. \n",
        "\n",
        "*Perhaps there is an easier tensorflow way of doing this that I haven't come across yet!?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6H6sVal_fPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Vertices\n",
        "vertices = []\n",
        "for point in mesh.points():\n",
        "  vertices.append(point)\n",
        "vertices = np.array(vertices, dtype=np.float32)\n",
        "\n",
        "# Load Triangles\n",
        "faces = []\n",
        "for fh in mesh.faces():\n",
        "  face = []\n",
        "  for vh in mesh.fv(fh):\n",
        "    face.append(vh.idx())\n",
        "  faces.append(face)\n",
        "faces = np.array(faces)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG6B0woWuJ2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_mesh_data(mesh_vertices):\n",
        "  mesh_data = {'vertices': mesh_vertices[:, ...], 'faces': faces[:, ...]}\n",
        "  return mesh_data\n",
        "\n",
        "mesh_viewer.Viewer(build_mesh_data(vertices))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBMcZ5pJ4RLm",
        "colab_type": "text"
      },
      "source": [
        "# Applying Shape Preserving Operations\n",
        "We will apply our simple shape preserving operations one by one on the original mesh. That is, these operations are applied independently. The result of one does not affect the next operation.\n",
        "\n",
        "Notice while apply our operations, we are altering only our vertices. That is, the operation applies straight to the vertices and the faces don't change. That said, I am not a fan of how I have to keep reconstructing the dictionary.\n",
        "\n",
        "Note that the camera is located at the same place throughout these changes. It is only the vertices that change positions.\n",
        "\n",
        "## Translation\n",
        "Here we try to translate the object by 1 unit in the x-axis (that is, to the right). The math is simple, <br>\n",
        "```\n",
        "new_position = current_position + distance * direction\n",
        "```\n",
        "\n",
        "In the matrix world, the translation matrix would look like\n",
        "```\n",
        "[1, 0, 0, x\n",
        " 0, 1, 0, y\n",
        " 0, 0, 1, z\n",
        " 0, 0, 0, 1]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3MnzPOvAMXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translation = 1.0 * np.array([1.0, 0.0, 0.0], dtype=np.float32)\n",
        "translated_vertices = vertices + translation\n",
        "\n",
        "mesh_viewer.Viewer(build_mesh_data(translated_vertices))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pquRlAqtuTjk",
        "colab_type": "text"
      },
      "source": [
        "## Rotation\n",
        "\n",
        "Rotation in computer graphics is done using Quaternions. Quaternions is a deep subject of its own and I highly recommend watching [this video](https://www.youtube.com/watch?v=zjMuIxRvygQ). It is a good quick intro to quaternions, but if interested you should check out [Visualizing Quaternions by Andrew J Hanson.](https://books.google.com/books/about/Visualizing_Quaternions.html?id=CoUB09xzme4C&printsec=frontcover&source=kp_read_button#v=onepage&q&f=false)\n",
        "\n",
        "In our example we try to rotate the model by 90 degrees (1.57 radians) counter clock-wise about the y-axis making the rover face us.\n",
        "\n",
        "The rotation matrix about the z axis would look like\n",
        "\n",
        "```\n",
        "[cos theta, -sin theta, 0, 0\n",
        " sin theta, cos theta , 0, 0\n",
        "    0    ,     0      , 1, 0\n",
        "    0    ,     0      , 0, 1]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBFRcs6XuWTw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rotation = quaternion.from_euler(np.array([0.0, 1.57, 0.0], dtype=np.float32))\n",
        "rotated_vertices = quaternion.rotate(vertices, rotation)\n",
        "\n",
        "mesh_viewer.Viewer(build_mesh_data(rotated_vertices))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLm_JrG8uXvI",
        "colab_type": "text"
      },
      "source": [
        "## Scaling\n",
        "\n",
        "We will perform what is called Uniform Scaling. That is, we will scale equally along the x-axis, y-axis and z-axis. Scaling matrix is generally represented as a diagonal matrix. That is, a matrix with 0s everywhere but the diagonal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htS0eOq8uZ-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scale = 1.5 * np.eye(3)  # np.eye produces an identity matrix\n",
        "scaled_vertices = np.transpose(np.matmul(scale, np.transpose(vertices)))  \n",
        "# Notice the double transpose. This is actually not needed.\n",
        "# We could have done np.matmul(vertices, scale).\n",
        "# But this does not play well with me because traditionally transformations ..\n",
        "# .. are to be pre-multiplied with the points. But because our vertices have ..\n",
        "# .. (x, 3) shape, I had to transpose it to get (3, x) shape for matrix ..\n",
        "# .. multiplication compatibility.\n",
        "mesh_viewer.Viewer(build_mesh_data(scaled_vertices))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqYRXlS-xeq7",
        "colab_type": "text"
      },
      "source": [
        "# Order Absolutely Matters\n",
        "\n",
        "With the following code I will demonstrate why order of operations matter when it comes to combining transformations.\n",
        "\n",
        "\n",
        "1.   First we will apply a rotation and then a translation. In matrix multiplication world this would look like\n",
        "```\n",
        "new_position = translation_matrix * rotation_matrix * current_position\n",
        "```\n",
        "2.   Then we will apply a translation followed by a rotation.\n",
        "```\n",
        "new_position = rotation_matrix * translation_matrix * current_position\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhd-pFbdulFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "order_1 = quaternion.rotate(vertices, rotation) + translation\n",
        "order_2 = quaternion.rotate(vertices + translation, rotation)\n",
        "mesh_viewer.Viewer(build_mesh_data(order_1))\n",
        "mesh_viewer.Viewer(build_mesh_data(order_2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVcM_6omz9Wv",
        "colab_type": "text"
      },
      "source": [
        "The easiest rule to remember is the **SRT rule.** You always apply scaling first, rotation next and finally translation. In matrix multiplication world the order is reversed. You would end up doing\n",
        "\n",
        "```\n",
        "new_position = translation * rotation * scaling * current_position\n",
        "```"
      ]
    }
  ]
}